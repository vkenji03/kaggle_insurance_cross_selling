# Binary Classification of Insurance Cross Selling (Kaggle competition)
This was my first Kaggle competition, where I achieved a rank of 1291 out of 2236 participants. For this project, I utilized Pandas, Dask, XGBoost, Optuna, Scikit-learn, and several other libraries.

## What I learned
- Gained hands-on experience with Pandas for handling large datasets
- Successfully employed Dask for efficient data processing
- Trained XGBoost models effectively using both Pandas and Dask DataFrames
- Used Optuna for hyperparameter optimization

## Chalenges faced
- Due to the large dataset size, training XGBoost models with Pandas encountered memory limitations. To overcome this, I adopted alternative approaches such as using the Dask library and implementing partial training with smaller data batches.

## Conclusion
This competition was a valuable learning experience. I acquired new skills, enjoyed the process, and am eager to tackle future challenges.
